{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdb17e33-0195-47b2-b7b2-d4815db0a20d",
   "metadata": {},
   "source": [
    "In Hugging Face, an encoder is a core component of many transformer-based models that processes input data—like text or images—into a rich, contextualized representation (digits or vectors).\n",
    "\n",
    "This representation can then be used for various downstream tasks such as classification, question answering, or as input to a decoder in sequence-to-sequence models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51357e4f-b3ca-4ff5-8852-2387a1b8e7d8",
   "metadata": {},
   "source": [
    "## Simulation of an encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f98a6651-a50c-4dbf-8e17-e54ab19b60b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Hi there , This is a beautiful day .'\n",
    "\n",
    "vocab = {\n",
    "    '<SOS>' : 0,\n",
    "    '<EOS>' : 1,\n",
    "    'Hi' : 3,\n",
    "    'there' : 4,\n",
    "    'This': 5,\n",
    "    'is' : 6,\n",
    "    'a' : 7,\n",
    "    'beautiful': 8,\n",
    "    'day' : 9,\n",
    "    ',' : 10,\n",
    "    '.' :11\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52482641-b7e9-4099-861f-fc9d6f69a6dc",
   "metadata": {},
   "source": [
    "In English, words are splitted by spaces. In Chinese, we can use jieba to split words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01dca279-aa1e-4412-9eb4-ddaa1e7ac678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> Hi there , This is a beautiful day . <EOS>\n"
     ]
    }
   ],
   "source": [
    "sent = '<SOS> ' + sentence + ' <EOS>'\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00afb817-38a3-4b81-8c04-67325b4d8b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', 'Hi', 'there', ',', 'This', 'is', 'a', 'beautiful', 'day', '.', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "words = sent.split()\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87d9dfe4-7f46-491b-99fa-20236c998fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3, 4, 10, 5, 6, 7, 8, 9, 11, 1]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[vocab[i] for i in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c74d96d-7704-42b3-a9b6-a3f7fd1b5b2c",
   "metadata": {},
   "source": [
    "## Use of the encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c0dac0-1c3d-4b24-975f-912f6b22400f",
   "metadata": {},
   "source": [
    "Models are encoders are in pairs. If you use a model, use its encoder accordingly.\n",
    "\n",
    "The encoder usually shares the same name as its model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "120f1838-7573-46a4-a9e4-a9b9b2aec414",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2239de82-5d0c-4680-81ec-4877bf1fe6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn on VPN and execute below code to connet the VPN network\n",
    "# import os\n",
    "# os.environ['https_proxy'] = 'XXX.XX.XX.XX'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33f1e19e-3f00-41a1-897d-306bf52ae882",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path = 'bert-base-chinese',\n",
    "    cache_dir = None,\n",
    "    force_download = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8141d63f-f64d-4c23-9a06-aad936ef6c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = ['我是一个小可爱。',\n",
    "        '我的裤腰很高。',\n",
    "        '我喜欢吃冰淇淋。',\n",
    "        '我喜欢开车。']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3f40d85-a674-4672-9284-04afde883aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2769, 3221, 671, 702, 2207, 1377, 4263, 511, 102, 2769, 4638, 6175, 5587, 2523, 7770, 511, 102, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "out = tokenizer.encode(\n",
    "    text = sents[0],\n",
    "    text_pair = sents[1],\n",
    "    truncation = True,\n",
    "    #is the sentence is not long enought, padding to maxlength\n",
    "    padding = 'max_length',\n",
    "    add_special_tokens = True,\n",
    "    max_length = 25,\n",
    "    return_tensors = None\n",
    ")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a180099-72c3-419b-bd79-cfe06f248764",
   "metadata": {},
   "source": [
    "`text`: This is your main input sentence. For example, sents[0] could be \"The sky is blue.\"\n",
    "\n",
    "`text_pair`: This is an optional second sentence that’s paired with the first, like sents[1] = \"The grass is green.\"\n",
    "\n",
    "When both are provided, the tokenizer will Encode them together as a single sequence and insert special separator tokens (like [SEP]) between them.\n",
    "\n",
    "You’d use it when your model needs to understand relationships between two sentences, such as:\n",
    "\n",
    "- Question and answer pairs\n",
    "- Premise and hypothesis in entailment tasks\n",
    "- Sentiment toward a specific topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ca13d70-6f94-436c-9499-080efb77621b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f7f3849-29fa-4665-a05f-0a7e9174b1a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 我 是 一 个 小 可 爱 。 [SEP] 我 的 裤 腰 很 高 。 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Revert the numbers back to string.\n",
    "tokenizer.decode(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d524b1a5-3fd0-4f95-8812-0eff30cc0fb6",
   "metadata": {},
   "source": [
    "We can see in Bert, each Chinese character is encoded to one number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9c9dba-4a32-42fc-b5f6-5fa679ab66f9",
   "metadata": {},
   "source": [
    "## Encoder plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3aad39f0-d8a5-438d-98aa-e2dc3efec0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "out2 = tokenizer.encode_plus(\n",
    "    text = sents[0],\n",
    "    text_pair = sents[1],\n",
    "    truncation = True,\n",
    "    padding = 'max_length',\n",
    "    max_length = 25,\n",
    "    add_special_tokens = True,\n",
    "    return_tensors = None,\n",
    "    #encoder plus parameters below\n",
    "    return_token_type_ids = True,\n",
    "    return_attention_mask = True,\n",
    "    return_special_tokens_mask = True,\n",
    "    return_length = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1384fd3b-e72b-4fc0-8fd8-5d80024b5ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2769, 3221, 671, 702, 2207, 1377, 4263, 511, 102, 2769, 4638, 6175, 5587, 2523, 7770, 511, 102, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], 'special_tokens_mask': [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], 'length': 25}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b99e256c-14d3-4f1a-83d3-c793ddb55cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids : [101, 2769, 3221, 671, 702, 2207, 1377, 4263, 511, 102, 2769, 4638, 6175, 5587, 2523, 7770, 511, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "token_type_ids : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "special_tokens_mask : [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "attention_mask : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "length : 25\n"
     ]
    }
   ],
   "source": [
    "for k, v in out2.items():\n",
    "    print(k, ':', v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40147570-5def-4886-8cfe-335ff4b7e8d5",
   "metadata": {},
   "source": [
    "### In encoder plus, several masks can be defined, more information will be revealed in the output. And the output will be a dictionary instead of a list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96702d70-f280-4b16-acee-63522a8edb54",
   "metadata": {},
   "source": [
    "## Batch encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "026b90ae-2fee-4694-b13f-9a39bb0a8d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "out3 = tokenizer.batch_encode_plus(\n",
    "    batch_text_or_text_pairs = [(sents[0], sents[1]), (sents[2], sents[3])], #put one pair into one tuple, all the tuples in one list\n",
    "    truncation = True,\n",
    "    padding = 'max_length',\n",
    "    max_length = 25,\n",
    "    add_special_tokens = True,\n",
    "    return_tensors = None,\n",
    "    #encoder plus parameters below\n",
    "    return_token_type_ids = True,\n",
    "    return_attention_mask = True,\n",
    "    return_special_tokens_mask = True,\n",
    "    return_length = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6748a7e-b9fb-4dba-a5ab-1cca27e094c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids : [[101, 2769, 3221, 671, 702, 2207, 1377, 4263, 511, 102, 2769, 4638, 6175, 5587, 2523, 7770, 511, 102, 0, 0, 0, 0, 0, 0, 0], [101, 2769, 1599, 3614, 1391, 1102, 3899, 3900, 511, 102, 2769, 1599, 3614, 2458, 6756, 511, 102, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "token_type_ids : [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "special_tokens_mask : [[1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "length : [18, 17]\n",
      "attention_mask : [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "for k, v in out3.items():\n",
    "    print(k, ':', v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59f924b-f1bb-4e33-8f83-672d592b8131",
   "metadata": {},
   "source": [
    "### In batch encoder, the output will be dictionary, and the value of the dictionary will be lists as it will cover more sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b35b6195-e4fd-4302-a001-bb7ff196e7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "out4 = tokenizer.batch_encode_plus(\n",
    "    batch_text_or_text_pairs = sents, #If there are not pairs, just simply use a list as input texts\n",
    "    truncation = True,\n",
    "    padding = 'max_length',\n",
    "    max_length = 25,\n",
    "    add_special_tokens = True,\n",
    "    return_tensors = None,\n",
    "    #encoder plus parameters below\n",
    "    return_token_type_ids = True,\n",
    "    return_attention_mask = True,\n",
    "    return_special_tokens_mask = True,\n",
    "    return_length = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9214f981-6cd7-48d1-bded-4bf94583e9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids : [[101, 2769, 3221, 671, 702, 2207, 1377, 4263, 511, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2769, 4638, 6175, 5587, 2523, 7770, 511, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2769, 1599, 3614, 1391, 1102, 3899, 3900, 511, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2769, 1599, 3614, 2458, 6756, 511, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "token_type_ids : [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "special_tokens_mask : [[1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "length : [10, 9, 10, 8]\n",
      "attention_mask : [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "for k, v in out4.items():\n",
    "    print(k, ':', v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7623830-00b4-4ba2-b7a6-75f67ef3ef6f",
   "metadata": {},
   "source": [
    "## Dictionary operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a85e0c5c-e899-48c9-a7e9-7d5ffb014c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e260b5c-82f5-4f3f-ab6e-aa1c90abc86c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[PAD]': 0,\n",
       " '[unused1]': 1,\n",
       " '[unused2]': 2,\n",
       " '[unused3]': 3,\n",
       " '[unused4]': 4,\n",
       " '[unused5]': 5,\n",
       " '[unused6]': 6,\n",
       " '[unused7]': 7,\n",
       " '[unused8]': 8,\n",
       " '[unused9]': 9,\n",
       " '[unused10]': 10,\n",
       " '[unused11]': 11,\n",
       " '[unused12]': 12,\n",
       " '[unused13]': 13,\n",
       " '[unused14]': 14,\n",
       " '[unused15]': 15,\n",
       " '[unused16]': 16,\n",
       " '[unused17]': 17,\n",
       " '[unused18]': 18,\n",
       " '[unused19]': 19,\n",
       " '[unused20]': 20,\n",
       " '[unused21]': 21,\n",
       " '[unused22]': 22,\n",
       " '[unused23]': 23,\n",
       " '[unused24]': 24,\n",
       " '[unused25]': 25,\n",
       " '[unused26]': 26,\n",
       " '[unused27]': 27,\n",
       " '[unused28]': 28,\n",
       " '[unused29]': 29,\n",
       " '[unused30]': 30,\n",
       " '[unused31]': 31,\n",
       " '[unused32]': 32,\n",
       " '[unused33]': 33,\n",
       " '[unused34]': 34,\n",
       " '[unused35]': 35,\n",
       " '[unused36]': 36,\n",
       " '[unused37]': 37,\n",
       " '[unused38]': 38,\n",
       " '[unused39]': 39,\n",
       " '[unused40]': 40,\n",
       " '[unused41]': 41,\n",
       " '[unused42]': 42,\n",
       " '[unused43]': 43,\n",
       " '[unused44]': 44,\n",
       " '[unused45]': 45,\n",
       " '[unused46]': 46,\n",
       " '[unused47]': 47,\n",
       " '[unused48]': 48,\n",
       " '[unused49]': 49,\n",
       " '[unused50]': 50,\n",
       " '[unused51]': 51,\n",
       " '[unused52]': 52,\n",
       " '[unused53]': 53,\n",
       " '[unused54]': 54,\n",
       " '[unused55]': 55,\n",
       " '[unused56]': 56,\n",
       " '[unused57]': 57,\n",
       " '[unused58]': 58,\n",
       " '[unused59]': 59,\n",
       " '[unused60]': 60,\n",
       " '[unused61]': 61,\n",
       " '[unused62]': 62,\n",
       " '[unused63]': 63,\n",
       " '[unused64]': 64,\n",
       " '[unused65]': 65,\n",
       " '[unused66]': 66,\n",
       " '[unused67]': 67,\n",
       " '[unused68]': 68,\n",
       " '[unused69]': 69,\n",
       " '[unused70]': 70,\n",
       " '[unused71]': 71,\n",
       " '[unused72]': 72,\n",
       " '[unused73]': 73,\n",
       " '[unused74]': 74,\n",
       " '[unused75]': 75,\n",
       " '[unused76]': 76,\n",
       " '[unused77]': 77,\n",
       " '[unused78]': 78,\n",
       " '[unused79]': 79,\n",
       " '[unused80]': 80,\n",
       " '[unused81]': 81,\n",
       " '[unused82]': 82,\n",
       " '[unused83]': 83,\n",
       " '[unused84]': 84,\n",
       " '[unused85]': 85,\n",
       " '[unused86]': 86,\n",
       " '[unused87]': 87,\n",
       " '[unused88]': 88,\n",
       " '[unused89]': 89,\n",
       " '[unused90]': 90,\n",
       " '[unused91]': 91,\n",
       " '[unused92]': 92,\n",
       " '[unused93]': 93,\n",
       " '[unused94]': 94,\n",
       " '[unused95]': 95,\n",
       " '[unused96]': 96,\n",
       " '[unused97]': 97,\n",
       " '[unused98]': 98,\n",
       " '[unused99]': 99,\n",
       " '[UNK]': 100,\n",
       " '[CLS]': 101,\n",
       " '[SEP]': 102,\n",
       " '[MASK]': 103,\n",
       " '<S>': 104,\n",
       " '<T>': 105,\n",
       " '!': 106,\n",
       " '\"': 107,\n",
       " '#': 108,\n",
       " '$': 109,\n",
       " '%': 110,\n",
       " '&': 111,\n",
       " \"'\": 112,\n",
       " '(': 113,\n",
       " ')': 114,\n",
       " '*': 115,\n",
       " '+': 116,\n",
       " ',': 117,\n",
       " '-': 118,\n",
       " '.': 119,\n",
       " '/': 120,\n",
       " '0': 121,\n",
       " '1': 122,\n",
       " '2': 123,\n",
       " '3': 124,\n",
       " '4': 125,\n",
       " '5': 126,\n",
       " '6': 127,\n",
       " '7': 128,\n",
       " '8': 129,\n",
       " '9': 130,\n",
       " ':': 131,\n",
       " ';': 132,\n",
       " '<': 133,\n",
       " '=': 134,\n",
       " '>': 135,\n",
       " '?': 136,\n",
       " '@': 137,\n",
       " '[': 138,\n",
       " '\\\\': 139,\n",
       " ']': 140,\n",
       " '^': 141,\n",
       " '_': 142,\n",
       " 'a': 143,\n",
       " 'b': 144,\n",
       " 'c': 145,\n",
       " 'd': 146,\n",
       " 'e': 147,\n",
       " 'f': 148,\n",
       " 'g': 149,\n",
       " 'h': 150,\n",
       " 'i': 151,\n",
       " 'j': 152,\n",
       " 'k': 153,\n",
       " 'l': 154,\n",
       " 'm': 155,\n",
       " 'n': 156,\n",
       " 'o': 157,\n",
       " 'p': 158,\n",
       " 'q': 159,\n",
       " 'r': 160,\n",
       " 's': 161,\n",
       " 't': 162,\n",
       " 'u': 163,\n",
       " 'v': 164,\n",
       " 'w': 165,\n",
       " 'x': 166,\n",
       " 'y': 167,\n",
       " 'z': 168,\n",
       " '{': 169,\n",
       " '|': 170,\n",
       " '}': 171,\n",
       " '~': 172,\n",
       " '£': 173,\n",
       " '¤': 174,\n",
       " '¥': 175,\n",
       " '§': 176,\n",
       " '©': 177,\n",
       " '«': 178,\n",
       " '®': 179,\n",
       " '°': 180,\n",
       " '±': 181,\n",
       " '²': 182,\n",
       " '³': 183,\n",
       " 'µ': 184,\n",
       " '·': 185,\n",
       " '¹': 186,\n",
       " 'º': 187,\n",
       " '»': 188,\n",
       " '¼': 189,\n",
       " '×': 190,\n",
       " 'ß': 191,\n",
       " 'æ': 192,\n",
       " '÷': 193,\n",
       " 'ø': 194,\n",
       " 'đ': 195,\n",
       " 'ŋ': 196,\n",
       " 'ɔ': 197,\n",
       " 'ə': 198,\n",
       " 'ɡ': 199,\n",
       " 'ʰ': 200,\n",
       " 'ˇ': 201,\n",
       " 'ˈ': 202,\n",
       " 'ˊ': 203,\n",
       " 'ˋ': 204,\n",
       " 'ˍ': 205,\n",
       " 'ː': 206,\n",
       " '˙': 207,\n",
       " '˚': 208,\n",
       " 'ˢ': 209,\n",
       " 'α': 210,\n",
       " 'β': 211,\n",
       " 'γ': 212,\n",
       " 'δ': 213,\n",
       " 'ε': 214,\n",
       " 'η': 215,\n",
       " 'θ': 216,\n",
       " 'ι': 217,\n",
       " 'κ': 218,\n",
       " 'λ': 219,\n",
       " 'μ': 220,\n",
       " 'ν': 221,\n",
       " 'ο': 222,\n",
       " 'π': 223,\n",
       " 'ρ': 224,\n",
       " 'ς': 225,\n",
       " 'σ': 226,\n",
       " 'τ': 227,\n",
       " 'υ': 228,\n",
       " 'φ': 229,\n",
       " 'χ': 230,\n",
       " 'ψ': 231,\n",
       " 'ω': 232,\n",
       " 'а': 233,\n",
       " 'б': 234,\n",
       " 'в': 235,\n",
       " 'г': 236,\n",
       " 'д': 237,\n",
       " 'е': 238,\n",
       " 'ж': 239,\n",
       " 'з': 240,\n",
       " 'и': 241,\n",
       " 'к': 242,\n",
       " 'л': 243,\n",
       " 'м': 244,\n",
       " 'н': 245,\n",
       " 'о': 246,\n",
       " 'п': 247,\n",
       " 'р': 248,\n",
       " 'с': 249,\n",
       " 'т': 250,\n",
       " 'у': 251,\n",
       " 'ф': 252,\n",
       " 'х': 253,\n",
       " 'ц': 254,\n",
       " 'ч': 255,\n",
       " 'ш': 256,\n",
       " 'ы': 257,\n",
       " 'ь': 258,\n",
       " 'я': 259,\n",
       " 'і': 260,\n",
       " 'ا': 261,\n",
       " 'ب': 262,\n",
       " 'ة': 263,\n",
       " 'ت': 264,\n",
       " 'د': 265,\n",
       " 'ر': 266,\n",
       " 'س': 267,\n",
       " 'ع': 268,\n",
       " 'ل': 269,\n",
       " 'م': 270,\n",
       " 'ن': 271,\n",
       " 'ه': 272,\n",
       " 'و': 273,\n",
       " 'ي': 274,\n",
       " '۩': 275,\n",
       " 'ก': 276,\n",
       " 'ง': 277,\n",
       " 'น': 278,\n",
       " 'ม': 279,\n",
       " 'ย': 280,\n",
       " 'ร': 281,\n",
       " 'อ': 282,\n",
       " 'า': 283,\n",
       " 'เ': 284,\n",
       " '๑': 285,\n",
       " '་': 286,\n",
       " 'ღ': 287,\n",
       " 'ᄀ': 288,\n",
       " 'ᄁ': 289,\n",
       " 'ᄂ': 290,\n",
       " 'ᄃ': 291,\n",
       " 'ᄅ': 292,\n",
       " 'ᄆ': 293,\n",
       " 'ᄇ': 294,\n",
       " 'ᄈ': 295,\n",
       " 'ᄉ': 296,\n",
       " 'ᄋ': 297,\n",
       " 'ᄌ': 298,\n",
       " 'ᄎ': 299,\n",
       " 'ᄏ': 300,\n",
       " 'ᄐ': 301,\n",
       " 'ᄑ': 302,\n",
       " 'ᄒ': 303,\n",
       " 'ᅡ': 304,\n",
       " 'ᅢ': 305,\n",
       " 'ᅣ': 306,\n",
       " 'ᅥ': 307,\n",
       " 'ᅦ': 308,\n",
       " 'ᅧ': 309,\n",
       " 'ᅨ': 310,\n",
       " 'ᅩ': 311,\n",
       " 'ᅪ': 312,\n",
       " 'ᅬ': 313,\n",
       " 'ᅭ': 314,\n",
       " 'ᅮ': 315,\n",
       " 'ᅯ': 316,\n",
       " 'ᅲ': 317,\n",
       " 'ᅳ': 318,\n",
       " 'ᅴ': 319,\n",
       " 'ᅵ': 320,\n",
       " 'ᆨ': 321,\n",
       " 'ᆫ': 322,\n",
       " 'ᆯ': 323,\n",
       " 'ᆷ': 324,\n",
       " 'ᆸ': 325,\n",
       " 'ᆺ': 326,\n",
       " 'ᆻ': 327,\n",
       " 'ᆼ': 328,\n",
       " 'ᗜ': 329,\n",
       " 'ᵃ': 330,\n",
       " 'ᵉ': 331,\n",
       " 'ᵍ': 332,\n",
       " 'ᵏ': 333,\n",
       " 'ᵐ': 334,\n",
       " 'ᵒ': 335,\n",
       " 'ᵘ': 336,\n",
       " '‖': 337,\n",
       " '„': 338,\n",
       " '†': 339,\n",
       " '•': 340,\n",
       " '‥': 341,\n",
       " '‧': 342,\n",
       " '\\u2028': 343,\n",
       " '‰': 344,\n",
       " '′': 345,\n",
       " '″': 346,\n",
       " '‹': 347,\n",
       " '›': 348,\n",
       " '※': 349,\n",
       " '‿': 350,\n",
       " '⁄': 351,\n",
       " 'ⁱ': 352,\n",
       " '⁺': 353,\n",
       " 'ⁿ': 354,\n",
       " '₁': 355,\n",
       " '₂': 356,\n",
       " '₃': 357,\n",
       " '₄': 358,\n",
       " '€': 359,\n",
       " '℃': 360,\n",
       " '№': 361,\n",
       " '™': 362,\n",
       " 'ⅰ': 363,\n",
       " 'ⅱ': 364,\n",
       " 'ⅲ': 365,\n",
       " 'ⅳ': 366,\n",
       " 'ⅴ': 367,\n",
       " '←': 368,\n",
       " '↑': 369,\n",
       " '→': 370,\n",
       " '↓': 371,\n",
       " '↔': 372,\n",
       " '↗': 373,\n",
       " '↘': 374,\n",
       " '⇒': 375,\n",
       " '∀': 376,\n",
       " '−': 377,\n",
       " '∕': 378,\n",
       " '∙': 379,\n",
       " '√': 380,\n",
       " '∞': 381,\n",
       " '∟': 382,\n",
       " '∠': 383,\n",
       " '∣': 384,\n",
       " '∥': 385,\n",
       " '∩': 386,\n",
       " '∮': 387,\n",
       " '∶': 388,\n",
       " '∼': 389,\n",
       " '∽': 390,\n",
       " '≈': 391,\n",
       " '≒': 392,\n",
       " '≡': 393,\n",
       " '≤': 394,\n",
       " '≥': 395,\n",
       " '≦': 396,\n",
       " '≧': 397,\n",
       " '≪': 398,\n",
       " '≫': 399,\n",
       " '⊙': 400,\n",
       " '⋅': 401,\n",
       " '⋈': 402,\n",
       " '⋯': 403,\n",
       " '⌒': 404,\n",
       " '①': 405,\n",
       " '②': 406,\n",
       " '③': 407,\n",
       " '④': 408,\n",
       " '⑤': 409,\n",
       " '⑥': 410,\n",
       " '⑦': 411,\n",
       " '⑧': 412,\n",
       " '⑨': 413,\n",
       " '⑩': 414,\n",
       " '⑴': 415,\n",
       " '⑵': 416,\n",
       " '⑶': 417,\n",
       " '⑷': 418,\n",
       " '⑸': 419,\n",
       " '⒈': 420,\n",
       " '⒉': 421,\n",
       " '⒊': 422,\n",
       " '⒋': 423,\n",
       " 'ⓒ': 424,\n",
       " 'ⓔ': 425,\n",
       " 'ⓘ': 426,\n",
       " '─': 427,\n",
       " '━': 428,\n",
       " '│': 429,\n",
       " '┃': 430,\n",
       " '┅': 431,\n",
       " '┆': 432,\n",
       " '┊': 433,\n",
       " '┌': 434,\n",
       " '└': 435,\n",
       " '├': 436,\n",
       " '┣': 437,\n",
       " '═': 438,\n",
       " '║': 439,\n",
       " '╚': 440,\n",
       " '╞': 441,\n",
       " '╠': 442,\n",
       " '╭': 443,\n",
       " '╮': 444,\n",
       " '╯': 445,\n",
       " '╰': 446,\n",
       " '╱': 447,\n",
       " '╳': 448,\n",
       " '▂': 449,\n",
       " '▃': 450,\n",
       " '▅': 451,\n",
       " '▇': 452,\n",
       " '█': 453,\n",
       " '▉': 454,\n",
       " '▋': 455,\n",
       " '▌': 456,\n",
       " '▍': 457,\n",
       " '▎': 458,\n",
       " '■': 459,\n",
       " '□': 460,\n",
       " '▪': 461,\n",
       " '▫': 462,\n",
       " '▬': 463,\n",
       " '▲': 464,\n",
       " '△': 465,\n",
       " '▶': 466,\n",
       " '►': 467,\n",
       " '▼': 468,\n",
       " '▽': 469,\n",
       " '◆': 470,\n",
       " '◇': 471,\n",
       " '○': 472,\n",
       " '◎': 473,\n",
       " '●': 474,\n",
       " '◕': 475,\n",
       " '◠': 476,\n",
       " '◢': 477,\n",
       " '◤': 478,\n",
       " '☀': 479,\n",
       " '★': 480,\n",
       " '☆': 481,\n",
       " '☕': 482,\n",
       " '☞': 483,\n",
       " '☺': 484,\n",
       " '☼': 485,\n",
       " '♀': 486,\n",
       " '♂': 487,\n",
       " '♠': 488,\n",
       " '♡': 489,\n",
       " '♣': 490,\n",
       " '♥': 491,\n",
       " '♦': 492,\n",
       " '♪': 493,\n",
       " '♫': 494,\n",
       " '♬': 495,\n",
       " '✈': 496,\n",
       " '✔': 497,\n",
       " '✕': 498,\n",
       " '✖': 499,\n",
       " '✦': 500,\n",
       " '✨': 501,\n",
       " '✪': 502,\n",
       " '✰': 503,\n",
       " '✿': 504,\n",
       " '❀': 505,\n",
       " '❤': 506,\n",
       " '➜': 507,\n",
       " '➤': 508,\n",
       " '⦿': 509,\n",
       " '、': 510,\n",
       " '。': 511,\n",
       " '〃': 512,\n",
       " '々': 513,\n",
       " '〇': 514,\n",
       " '〈': 515,\n",
       " '〉': 516,\n",
       " '《': 517,\n",
       " '》': 518,\n",
       " '「': 519,\n",
       " '」': 520,\n",
       " '『': 521,\n",
       " '』': 522,\n",
       " '【': 523,\n",
       " '】': 524,\n",
       " '〓': 525,\n",
       " '〔': 526,\n",
       " '〕': 527,\n",
       " '〖': 528,\n",
       " '〗': 529,\n",
       " '〜': 530,\n",
       " '〝': 531,\n",
       " '〞': 532,\n",
       " 'ぁ': 533,\n",
       " 'あ': 534,\n",
       " 'ぃ': 535,\n",
       " 'い': 536,\n",
       " 'う': 537,\n",
       " 'ぇ': 538,\n",
       " 'え': 539,\n",
       " 'お': 540,\n",
       " 'か': 541,\n",
       " 'き': 542,\n",
       " 'く': 543,\n",
       " 'け': 544,\n",
       " 'こ': 545,\n",
       " 'さ': 546,\n",
       " 'し': 547,\n",
       " 'す': 548,\n",
       " 'せ': 549,\n",
       " 'そ': 550,\n",
       " 'た': 551,\n",
       " 'ち': 552,\n",
       " 'っ': 553,\n",
       " 'つ': 554,\n",
       " 'て': 555,\n",
       " 'と': 556,\n",
       " 'な': 557,\n",
       " 'に': 558,\n",
       " 'ぬ': 559,\n",
       " 'ね': 560,\n",
       " 'の': 561,\n",
       " 'は': 562,\n",
       " 'ひ': 563,\n",
       " 'ふ': 564,\n",
       " 'へ': 565,\n",
       " 'ほ': 566,\n",
       " 'ま': 567,\n",
       " 'み': 568,\n",
       " 'む': 569,\n",
       " 'め': 570,\n",
       " 'も': 571,\n",
       " 'ゃ': 572,\n",
       " 'や': 573,\n",
       " 'ゅ': 574,\n",
       " 'ゆ': 575,\n",
       " 'ょ': 576,\n",
       " 'よ': 577,\n",
       " 'ら': 578,\n",
       " 'り': 579,\n",
       " 'る': 580,\n",
       " 'れ': 581,\n",
       " 'ろ': 582,\n",
       " 'わ': 583,\n",
       " 'を': 584,\n",
       " 'ん': 585,\n",
       " '゜': 586,\n",
       " 'ゝ': 587,\n",
       " 'ァ': 588,\n",
       " 'ア': 589,\n",
       " 'ィ': 590,\n",
       " 'イ': 591,\n",
       " 'ゥ': 592,\n",
       " 'ウ': 593,\n",
       " 'ェ': 594,\n",
       " 'エ': 595,\n",
       " 'ォ': 596,\n",
       " 'オ': 597,\n",
       " 'カ': 598,\n",
       " 'キ': 599,\n",
       " 'ク': 600,\n",
       " 'ケ': 601,\n",
       " 'コ': 602,\n",
       " 'サ': 603,\n",
       " 'シ': 604,\n",
       " 'ス': 605,\n",
       " 'セ': 606,\n",
       " 'ソ': 607,\n",
       " 'タ': 608,\n",
       " 'チ': 609,\n",
       " 'ッ': 610,\n",
       " 'ツ': 611,\n",
       " 'テ': 612,\n",
       " 'ト': 613,\n",
       " 'ナ': 614,\n",
       " 'ニ': 615,\n",
       " 'ヌ': 616,\n",
       " 'ネ': 617,\n",
       " 'ノ': 618,\n",
       " 'ハ': 619,\n",
       " 'ヒ': 620,\n",
       " 'フ': 621,\n",
       " 'ヘ': 622,\n",
       " 'ホ': 623,\n",
       " 'マ': 624,\n",
       " 'ミ': 625,\n",
       " 'ム': 626,\n",
       " 'メ': 627,\n",
       " 'モ': 628,\n",
       " 'ャ': 629,\n",
       " 'ヤ': 630,\n",
       " 'ュ': 631,\n",
       " 'ユ': 632,\n",
       " 'ョ': 633,\n",
       " 'ヨ': 634,\n",
       " 'ラ': 635,\n",
       " 'リ': 636,\n",
       " 'ル': 637,\n",
       " 'レ': 638,\n",
       " 'ロ': 639,\n",
       " 'ワ': 640,\n",
       " 'ヲ': 641,\n",
       " 'ン': 642,\n",
       " 'ヶ': 643,\n",
       " '・': 644,\n",
       " 'ー': 645,\n",
       " 'ヽ': 646,\n",
       " 'ㄅ': 647,\n",
       " 'ㄆ': 648,\n",
       " 'ㄇ': 649,\n",
       " 'ㄉ': 650,\n",
       " 'ㄋ': 651,\n",
       " 'ㄌ': 652,\n",
       " 'ㄍ': 653,\n",
       " 'ㄎ': 654,\n",
       " 'ㄏ': 655,\n",
       " 'ㄒ': 656,\n",
       " 'ㄚ': 657,\n",
       " 'ㄛ': 658,\n",
       " 'ㄞ': 659,\n",
       " 'ㄟ': 660,\n",
       " 'ㄢ': 661,\n",
       " 'ㄤ': 662,\n",
       " 'ㄥ': 663,\n",
       " 'ㄧ': 664,\n",
       " 'ㄨ': 665,\n",
       " 'ㆍ': 666,\n",
       " '㈦': 667,\n",
       " '㊣': 668,\n",
       " '㎡': 669,\n",
       " '㗎': 670,\n",
       " '一': 671,\n",
       " '丁': 672,\n",
       " '七': 673,\n",
       " '万': 674,\n",
       " '丈': 675,\n",
       " '三': 676,\n",
       " '上': 677,\n",
       " '下': 678,\n",
       " '不': 679,\n",
       " '与': 680,\n",
       " '丐': 681,\n",
       " '丑': 682,\n",
       " '专': 683,\n",
       " '且': 684,\n",
       " '丕': 685,\n",
       " '世': 686,\n",
       " '丘': 687,\n",
       " '丙': 688,\n",
       " '业': 689,\n",
       " '丛': 690,\n",
       " '东': 691,\n",
       " '丝': 692,\n",
       " '丞': 693,\n",
       " '丟': 694,\n",
       " '両': 695,\n",
       " '丢': 696,\n",
       " '两': 697,\n",
       " '严': 698,\n",
       " '並': 699,\n",
       " '丧': 700,\n",
       " '丨': 701,\n",
       " '个': 702,\n",
       " '丫': 703,\n",
       " '中': 704,\n",
       " '丰': 705,\n",
       " '串': 706,\n",
       " '临': 707,\n",
       " '丶': 708,\n",
       " '丸': 709,\n",
       " '丹': 710,\n",
       " '为': 711,\n",
       " '主': 712,\n",
       " '丼': 713,\n",
       " '丽': 714,\n",
       " '举': 715,\n",
       " '丿': 716,\n",
       " '乂': 717,\n",
       " '乃': 718,\n",
       " '久': 719,\n",
       " '么': 720,\n",
       " '义': 721,\n",
       " '之': 722,\n",
       " '乌': 723,\n",
       " '乍': 724,\n",
       " '乎': 725,\n",
       " '乏': 726,\n",
       " '乐': 727,\n",
       " '乒': 728,\n",
       " '乓': 729,\n",
       " '乔': 730,\n",
       " '乖': 731,\n",
       " '乗': 732,\n",
       " '乘': 733,\n",
       " '乙': 734,\n",
       " '乜': 735,\n",
       " '九': 736,\n",
       " '乞': 737,\n",
       " '也': 738,\n",
       " '习': 739,\n",
       " '乡': 740,\n",
       " '书': 741,\n",
       " '乩': 742,\n",
       " '买': 743,\n",
       " '乱': 744,\n",
       " '乳': 745,\n",
       " '乾': 746,\n",
       " '亀': 747,\n",
       " '亂': 748,\n",
       " '了': 749,\n",
       " '予': 750,\n",
       " '争': 751,\n",
       " '事': 752,\n",
       " '二': 753,\n",
       " '于': 754,\n",
       " '亏': 755,\n",
       " '云': 756,\n",
       " '互': 757,\n",
       " '五': 758,\n",
       " '井': 759,\n",
       " '亘': 760,\n",
       " '亙': 761,\n",
       " '亚': 762,\n",
       " '些': 763,\n",
       " '亜': 764,\n",
       " '亞': 765,\n",
       " '亟': 766,\n",
       " '亡': 767,\n",
       " '亢': 768,\n",
       " '交': 769,\n",
       " '亥': 770,\n",
       " '亦': 771,\n",
       " '产': 772,\n",
       " '亨': 773,\n",
       " '亩': 774,\n",
       " '享': 775,\n",
       " '京': 776,\n",
       " '亭': 777,\n",
       " '亮': 778,\n",
       " '亲': 779,\n",
       " '亳': 780,\n",
       " '亵': 781,\n",
       " '人': 782,\n",
       " '亿': 783,\n",
       " '什': 784,\n",
       " '仁': 785,\n",
       " '仃': 786,\n",
       " '仄': 787,\n",
       " '仅': 788,\n",
       " '仆': 789,\n",
       " '仇': 790,\n",
       " '今': 791,\n",
       " '介': 792,\n",
       " '仍': 793,\n",
       " '从': 794,\n",
       " '仏': 795,\n",
       " '仑': 796,\n",
       " '仓': 797,\n",
       " '仔': 798,\n",
       " '仕': 799,\n",
       " '他': 800,\n",
       " '仗': 801,\n",
       " '付': 802,\n",
       " '仙': 803,\n",
       " '仝': 804,\n",
       " '仞': 805,\n",
       " '仟': 806,\n",
       " '代': 807,\n",
       " '令': 808,\n",
       " '以': 809,\n",
       " '仨': 810,\n",
       " '仪': 811,\n",
       " '们': 812,\n",
       " '仮': 813,\n",
       " '仰': 814,\n",
       " '仲': 815,\n",
       " '件': 816,\n",
       " '价': 817,\n",
       " '任': 818,\n",
       " '份': 819,\n",
       " '仿': 820,\n",
       " '企': 821,\n",
       " '伉': 822,\n",
       " '伊': 823,\n",
       " '伍': 824,\n",
       " '伎': 825,\n",
       " '伏': 826,\n",
       " '伐': 827,\n",
       " '休': 828,\n",
       " '伕': 829,\n",
       " '众': 830,\n",
       " '优': 831,\n",
       " '伙': 832,\n",
       " '会': 833,\n",
       " '伝': 834,\n",
       " '伞': 835,\n",
       " '伟': 836,\n",
       " '传': 837,\n",
       " '伢': 838,\n",
       " '伤': 839,\n",
       " '伦': 840,\n",
       " '伪': 841,\n",
       " '伫': 842,\n",
       " '伯': 843,\n",
       " '估': 844,\n",
       " '伴': 845,\n",
       " '伶': 846,\n",
       " '伸': 847,\n",
       " '伺': 848,\n",
       " '似': 849,\n",
       " '伽': 850,\n",
       " '佃': 851,\n",
       " '但': 852,\n",
       " '佇': 853,\n",
       " '佈': 854,\n",
       " '位': 855,\n",
       " '低': 856,\n",
       " '住': 857,\n",
       " '佐': 858,\n",
       " '佑': 859,\n",
       " '体': 860,\n",
       " '佔': 861,\n",
       " '何': 862,\n",
       " '佗': 863,\n",
       " '佘': 864,\n",
       " '余': 865,\n",
       " '佚': 866,\n",
       " '佛': 867,\n",
       " '作': 868,\n",
       " '佝': 869,\n",
       " '佞': 870,\n",
       " '佟': 871,\n",
       " '你': 872,\n",
       " '佢': 873,\n",
       " '佣': 874,\n",
       " '佤': 875,\n",
       " '佥': 876,\n",
       " '佩': 877,\n",
       " '佬': 878,\n",
       " '佯': 879,\n",
       " '佰': 880,\n",
       " '佳': 881,\n",
       " '併': 882,\n",
       " '佶': 883,\n",
       " '佻': 884,\n",
       " '佼': 885,\n",
       " '使': 886,\n",
       " '侃': 887,\n",
       " '侄': 888,\n",
       " '來': 889,\n",
       " '侈': 890,\n",
       " '例': 891,\n",
       " '侍': 892,\n",
       " '侏': 893,\n",
       " '侑': 894,\n",
       " '侖': 895,\n",
       " '侗': 896,\n",
       " '供': 897,\n",
       " '依': 898,\n",
       " '侠': 899,\n",
       " '価': 900,\n",
       " '侣': 901,\n",
       " '侥': 902,\n",
       " '侦': 903,\n",
       " '侧': 904,\n",
       " '侨': 905,\n",
       " '侬': 906,\n",
       " '侮': 907,\n",
       " '侯': 908,\n",
       " '侵': 909,\n",
       " '侶': 910,\n",
       " '侷': 911,\n",
       " '便': 912,\n",
       " '係': 913,\n",
       " '促': 914,\n",
       " '俄': 915,\n",
       " '俊': 916,\n",
       " '俎': 917,\n",
       " '俏': 918,\n",
       " '俐': 919,\n",
       " '俑': 920,\n",
       " '俗': 921,\n",
       " '俘': 922,\n",
       " '俚': 923,\n",
       " '保': 924,\n",
       " '俞': 925,\n",
       " '俟': 926,\n",
       " '俠': 927,\n",
       " '信': 928,\n",
       " '俨': 929,\n",
       " '俩': 930,\n",
       " '俪': 931,\n",
       " '俬': 932,\n",
       " '俭': 933,\n",
       " '修': 934,\n",
       " '俯': 935,\n",
       " '俱': 936,\n",
       " '俳': 937,\n",
       " '俸': 938,\n",
       " '俺': 939,\n",
       " '俾': 940,\n",
       " '倆': 941,\n",
       " '倉': 942,\n",
       " '個': 943,\n",
       " '倌': 944,\n",
       " '倍': 945,\n",
       " '倏': 946,\n",
       " '們': 947,\n",
       " '倒': 948,\n",
       " '倔': 949,\n",
       " '倖': 950,\n",
       " '倘': 951,\n",
       " '候': 952,\n",
       " '倚': 953,\n",
       " '倜': 954,\n",
       " '借': 955,\n",
       " '倡': 956,\n",
       " '値': 957,\n",
       " '倦': 958,\n",
       " '倩': 959,\n",
       " '倪': 960,\n",
       " '倫': 961,\n",
       " '倬': 962,\n",
       " '倭': 963,\n",
       " '倶': 964,\n",
       " '债': 965,\n",
       " '值': 966,\n",
       " '倾': 967,\n",
       " '偃': 968,\n",
       " '假': 969,\n",
       " '偈': 970,\n",
       " '偉': 971,\n",
       " '偌': 972,\n",
       " '偎': 973,\n",
       " '偏': 974,\n",
       " '偕': 975,\n",
       " '做': 976,\n",
       " '停': 977,\n",
       " '健': 978,\n",
       " '側': 979,\n",
       " '偵': 980,\n",
       " '偶': 981,\n",
       " '偷': 982,\n",
       " '偻': 983,\n",
       " '偽': 984,\n",
       " '偿': 985,\n",
       " '傀': 986,\n",
       " '傅': 987,\n",
       " '傍': 988,\n",
       " '傑': 989,\n",
       " '傘': 990,\n",
       " '備': 991,\n",
       " '傚': 992,\n",
       " '傢': 993,\n",
       " '傣': 994,\n",
       " '傥': 995,\n",
       " '储': 996,\n",
       " '傩': 997,\n",
       " '催': 998,\n",
       " '傭': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a5670a6f-1746-4a97-a485-fcb37236a245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21128"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a500f0-3f41-476b-9d83-ea6e0afa216c",
   "metadata": {},
   "source": [
    "By default, **bert_base_chinese** has over 20K words encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "06a85251-4a94-4d94-a4ea-6f864d32ad04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bert_base_chinese treat a single Chinese character as a word\n",
    "'心态' in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f3344c98-c4a5-41db-9793-b558e8016d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'态' in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "94f26192-05c8-42ed-88d2-dc2641bed0fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add new tokens\n",
    "tokenizer.add_tokens(new_tokens =['心态', '正规', '权威'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "38d6a320-95f6-4d69-8081-690356acf4d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add special character\n",
    "tokenizer.add_special_tokens({'eos_token': '[EOS]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "faa6263b-17e7-4362-91e9-99f8ae3d6cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21128\n",
      "21129\n",
      "21130\n",
      "21131\n"
     ]
    }
   ],
   "source": [
    "for word in ['心态', '正规', '权威','[EOS]']:\n",
    "    print(tokenizer.get_vocab()[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f8f2c8-5698-4d2a-956c-969ad0772d5a",
   "metadata": {},
   "source": [
    "Use the updated vacabulary to encode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "08ab15a1-b090-4424-b1f7-9512c897aee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "out5 = tokenizer.encode(\n",
    "    text = '他的心态很正规[EOS]',\n",
    "    truncation = True,\n",
    "    padding = 'max_length',\n",
    "    max_length = 10,\n",
    "    add_special_tokens = True,\n",
    "    return_tensors = None,\n",
    "    #encoder plus parameters below\n",
    "    return_token_type_ids = True,\n",
    "    return_attention_mask = True,\n",
    "    return_special_tokens_mask = True,\n",
    "    return_length = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "51b5dc4b-7d55-471e-b8cd-5210a3a54f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 800, 4638, 21128, 2523, 21129, 21131, 102, 0, 0]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "057a9819-2a25-49e3-8475-3969be7c7223",
   "metadata": {},
   "outputs": [],
   "source": [
    "out6 = tokenizer.encode_plus(\n",
    "    text = '他的心态很正规[EOS]',\n",
    "    truncation = True,\n",
    "    padding = 'max_length',\n",
    "    max_length = 10,\n",
    "    add_special_tokens = True,\n",
    "    return_tensors = None,\n",
    "    #encoder plus parameters below\n",
    "    return_token_type_ids = True,\n",
    "    return_attention_mask = True,\n",
    "    return_special_tokens_mask = True,\n",
    "    return_length = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "01534d4a-6b01-4f0f-ab33-b32600a79a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 800, 4638, 21128, 2523, 21129, 21131, 102, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'special_tokens_mask': [1, 0, 0, 0, 0, 0, 0, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0], 'length': 10}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9ae5bc60-bf01-4f1c-bd12-48d06e355c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 他 的 心态 很 正规 [EOS] [SEP] [PAD] [PAD]'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(out5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cd2dc036-cf43-4433-a03d-114783108750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 他 的 心态 很 正规 [EOS] [SEP] [PAD] [PAD]'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(out6['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8911af52-7b6f-4636-b21f-f6dce23f9d54",
   "metadata": {},
   "source": [
    "## Workflow of encoder:\n",
    "- defining dictionary\n",
    "- preprocessing\n",
    "- tokenization\n",
    "- encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cf61d3-5b9f-4fe0-b61a-0db685c0e755",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
