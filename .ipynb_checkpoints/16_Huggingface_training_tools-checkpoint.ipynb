{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05dd1df4-d301-486e-ba1c-4af492dc52d1",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f42b923a-394c-4577-afda-987a63800d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aa1117c790d4644a793dc54fc8fbebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/19.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7644e8f614d4407b09a007c2bf3094d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/828 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dfd4a7c8937480c8f742c8ff3d6a52f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3566482e5cba4e4e9c4941ae2e4c0de0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f8d61c827554481be9cd6e02350c6ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a3a671caea4a57bca294cf9a9e9b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'hfl/rbt3'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7919da3-9ab1-4ab8-a0b2-861303fc2ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='hfl/rbt3', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bdb20c5-628b-400c-b8ea-b107eedb9037",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 3217, 1921, 3221, 1391, 1102, 3899, 1119, 4638, 3198, 5688, 102], [101, 2769, 1762, 1599, 7716, 2861, 7414, 2255, 4638, 677, 4958, 2835, 5291, 7607, 3322, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trail\n",
    "tokenizer.batch_encode_plus(['春天是吃冰淇凌的时节', '我在喜马拉雅山的上空折纸飞机'], truncation = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fbde20b-c842-499e-aa0b-93441b46a113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk('../ChnSentiCorp/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c508943-0352-47d5-a0ed-13cc5757fa55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 9600\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 0\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b70dea6-a75a-4ee6-9168-86280ad58007",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'] = dataset['train'].shuffle().select(range(2000))\n",
    "dataset['test'] = dataset['test'].shuffle().select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f77f599d-d78b-4206-95de-5c0e614a4e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '散热确实不太好。显卡说是3450装上驱动显示是3400。没有同方服务金卡，只有一个电池的延保卡。要是再送包和鼠标就更好了。',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de59c1dd-b3a5-4dc6-aab8-ce020885069e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(data, tokenizer):\n",
    "    return tokenizer.batch_encode_plus(data['text'], truncation = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d892353-859a-41e2-a4ba-de41b6d3a7df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee4301fbd4b340f38fdd4b3e02e6ef67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d38d8542d8fd4a268b24dbef25a5fae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(f,\n",
    "                      batched = True,\n",
    "                      batch_size = 1000,\n",
    "                      num_proc = 4,\n",
    "                      remove_columns = ['text'],\n",
    "                      fn_kwargs = {'tokenizer':tokenizer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfa4e2f4-0225-40d1-a8ef-a42b8ecd3c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['label'],\n",
       "        num_rows: 0\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfc9e6b9-dd33-4b0d-85be-6f042a96072d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 0,\n",
       " 'input_ids': [101,\n",
       "  3141,\n",
       "  4178,\n",
       "  4802,\n",
       "  2141,\n",
       "  679,\n",
       "  1922,\n",
       "  1962,\n",
       "  511,\n",
       "  3227,\n",
       "  1305,\n",
       "  6432,\n",
       "  3221,\n",
       "  11434,\n",
       "  8129,\n",
       "  6163,\n",
       "  677,\n",
       "  7721,\n",
       "  1220,\n",
       "  3227,\n",
       "  4850,\n",
       "  3221,\n",
       "  13294,\n",
       "  511,\n",
       "  3766,\n",
       "  3300,\n",
       "  1398,\n",
       "  3175,\n",
       "  3302,\n",
       "  1218,\n",
       "  7032,\n",
       "  1305,\n",
       "  8024,\n",
       "  1372,\n",
       "  3300,\n",
       "  671,\n",
       "  702,\n",
       "  4510,\n",
       "  3737,\n",
       "  4638,\n",
       "  2454,\n",
       "  924,\n",
       "  1305,\n",
       "  511,\n",
       "  6206,\n",
       "  3221,\n",
       "  1086,\n",
       "  6843,\n",
       "  1259,\n",
       "  1469,\n",
       "  7962,\n",
       "  3403,\n",
       "  2218,\n",
       "  3291,\n",
       "  1962,\n",
       "  749,\n",
       "  511,\n",
       "  102],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0cd4546-5eb2-4544-bca8-9955678366d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1272c63448474ef2b7a71ac998b25c32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "122b14c327d34df6a08a0b8793e2e3e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# delete long sentences\n",
    "def f2(data):\n",
    "    return [len(i) <=  512 for i in data['input_ids']]\n",
    "\n",
    "dataset = dataset.filter(\n",
    "    f2,\n",
    "    batched = True,\n",
    "    batch_size = 1000,\n",
    "    num_proc = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3c770f-d783-40a8-9fb8-a8c72a9b46b7",
   "metadata": {},
   "source": [
    "`batch_size` doesn’t change what gets filtered (the output result will be the same) —it just controls how efficiently the filtering happens.\n",
    "- It affects memory usage: Larger batches use more RAM.\n",
    "- It affects speed: Larger batches usually mean fewer function calls and faster processing.\n",
    "- It affects parallelism: With num_proc=4, each of the 4 processes will handle batches of 1000 examples independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c2b7648-09c0-403e-9f9c-f84d760ff0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7a3ff70-32d0-415e-b6ae-05d2abc8d1dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "646a5520c606440f976c29c37e33327b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/156M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/rbt3 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('hfl/rbt3', num_labels = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4f91406-55b0-4db4-9adc-de754a166521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38478338"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([i.nelement() for i in model.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae1b6b6-988c-465c-82e5-0a2221d5acfe",
   "metadata": {},
   "source": [
    "There are about 38 millions parameters in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87523c53-4200-43d9-814c-7db2e6ae4006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ceb28e59-45b6-4b41-b08a-489cf9b1fa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trail\n",
    "data = {\n",
    "    'input_ids': torch.ones(4,10, dtype = torch.long),\n",
    "    'token_type_ids': torch.ones(4,10, dtype = torch.long),\n",
    "    'attention_mask':torch.ones(4,10, dtype = torch.long),\n",
    "    'labels':torch.ones(4, dtype = torch.long),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "076cee09-d510-4608-a932-1fd9bb44e3de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " 'token_type_ids': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " 'labels': tensor([1, 1, 1, 1])}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "638ab214-eb9d-4dd4-b58d-24ac44e474dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(**data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09dfb829-af87-4c5e-b09c-fc67203836fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.9219, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3891, -0.0256],\n",
       "        [ 0.3891, -0.0256],\n",
       "        [ 0.3891, -0.0256],\n",
       "        [ 0.3891, -0.0256]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5557ea15-6044-47dd-9bc5-8ea16aa677a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9219, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a5f09438-b4a8-49c8-9f9a-be047438485c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3891, -0.0256],\n",
       "        [ 0.3891, -0.0256],\n",
       "        [ 0.3891, -0.0256],\n",
       "        [ 0.3891, -0.0256]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['logits']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1867139-7e11-4a2b-b529-93bed3149ee9",
   "metadata": {},
   "source": [
    "## Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "82e77331-e3c7-4884-90a5-8ae83db16598",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load as load_metric\n",
    "metric = load_metric('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "310c9261-5063-40f8-8058-0ac0868abc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers.trainer_utils import EvalPrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0b122c67-013c-4283-8477-4e9d160312ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    pred = logits.argmax(axis = 1)\n",
    "    print (pred)\n",
    "    return metric.compute(predictions = pred, references = labels)\n",
    "    # return {'accuracy' : (pred == labels).mean()}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ce3a392b-6c05-498e-a89c-c5ccddf79c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_pred = EvalPrediction(\n",
    "    predictions = np.array([[1,0],[9,3],[0.4,0.3],[6,7]]),\n",
    "    label_ids = np.array([1,1,0,1])\n",
    ")\n",
    "compute_metrics(eval_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e2b7f390-7c05-4fa3-a8e7-1299619afd57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.trainer_utils.EvalPrediction at 0x156b18a30>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688f7987-68c1-4e83-8603-a8d1206a4012",
   "metadata": {},
   "source": [
    "## Parameter in training\n",
    "In Huggingface, the parameters are packed in a **class -- TrainingArguments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "905cb2b9-ef0d-4e77-95c5-5b4c67b51cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "550acebc-40a7-46b2-a535-c818db5ae9c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./output_dir\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# temporary output data's saving path\u001b[39;49;00m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# evaluation_strategy = 'steps',\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m#how many steps to perform an evaluation\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msteps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# it can be 'no', 'epoch', 'steps'\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#𝜆 to multiply on the sum of square of weights in loss function, to reduce overfitting\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# no_cuda = False\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:133\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, hub_revision, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, liger_kernel_config, eval_use_gather_object, average_tokens_across_devices)\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Advanced_AI/lib/python3.10/site-packages/transformers/training_args.py:1790\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1788\u001b[0m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[0;32m-> 1790\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m   1792\u001b[0m \u001b[38;5;66;03m# Disable average tokens when using single device\u001b[39;00m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maverage_tokens_across_devices:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Advanced_AI/lib/python3.10/site-packages/transformers/training_args.py:2319\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2315\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2316\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[1;32m   2317\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2318\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m-> 2319\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_devices\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Advanced_AI/lib/python3.10/site-packages/transformers/utils/generic.py:67\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     65\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 67\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Advanced_AI/lib/python3.10/site-packages/transformers/training_args.py:2189\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[1;32m   2188\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[0;32m-> 2189\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   2190\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2191\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease run `pip install transformers[torch]` or `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2192\u001b[0m         )\n\u001b[1;32m   2193\u001b[0m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[1;32m   2194\u001b[0m accelerator_state_kwargs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menabled\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_configured_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
      "\u001b[0;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir = './output_dir', # temporary output data's saving path\n",
    "    # evaluation_strategy = 'steps',\n",
    "    \n",
    "    eval_steps = 30,#how many steps to perform an evaluation\n",
    "    save_strategy = 'steps', # it can be 'no', 'epoch', 'steps'\n",
    "    save_steps = 30,\n",
    "\n",
    "    num_train_epochs = 2,\n",
    "\n",
    "    learning_rate = 0.001,\n",
    "\n",
    "    weight_decay = 0.01, #𝜆 to multiply on the sum of square of weights in loss function, to reduce overfitting\n",
    "\n",
    "    per_device_eval_batch_size = 16,\n",
    "    per_device_train_batch_size = 16,\n",
    "\n",
    "    # no_cuda = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1bb9e68a-f045-4269-a17e-1c6cba13066b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1\n"
     ]
    }
   ],
   "source": [
    "import accelerate\n",
    "print(accelerate.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "00e168ca-6709-42ba-ba95-598275ab6103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate>=0.26.0\n",
      "  Using cached accelerate-1.8.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting numpy<3.0.0,>=1.17 (from accelerate>=0.26.0)\n",
      "  Using cached numpy-2.2.6-cp310-cp310-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting packaging>=20.0 (from accelerate>=0.26.0)\n",
      "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting psutil (from accelerate>=0.26.0)\n",
      "  Using cached psutil-7.0.0-cp36-abi3-macosx_11_0_arm64.whl.metadata (22 kB)\n",
      "Collecting pyyaml (from accelerate>=0.26.0)\n",
      "  Using cached PyYAML-6.0.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting torch>=2.0.0 (from accelerate>=0.26.0)\n",
      "  Using cached torch-2.7.1-cp310-none-macosx_11_0_arm64.whl.metadata (29 kB)\n",
      "Collecting huggingface_hub>=0.21.0 (from accelerate>=0.26.0)\n",
      "  Using cached huggingface_hub-0.33.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting safetensors>=0.4.3 (from accelerate>=0.26.0)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting filelock (from huggingface_hub>=0.21.0->accelerate>=0.26.0)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface_hub>=0.21.0->accelerate>=0.26.0)\n",
      "  Using cached fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting requests (from huggingface_hub>=0.21.0->accelerate>=0.26.0)\n",
      "  Using cached requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tqdm>=4.42.1 (from huggingface_hub>=0.21.0->accelerate>=0.26.0)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface_hub>=0.21.0->accelerate>=0.26.0)\n",
      "  Using cached typing_extensions-4.14.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface_hub>=0.21.0->accelerate>=0.26.0)\n",
      "  Using cached hf_xet-1.1.5-cp37-abi3-macosx_11_0_arm64.whl.metadata (879 bytes)\n",
      "Collecting sympy>=1.13.3 (from torch>=2.0.0->accelerate>=0.26.0)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=2.0.0->accelerate>=0.26.0)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch>=2.0.0->accelerate>=0.26.0)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=2.0.0->accelerate>=0.26.0)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=2.0.0->accelerate>=0.26.0)\n",
      "  Using cached MarkupSafe-3.0.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0)\n",
      "  Using cached charset_normalizer-3.4.2-cp310-cp310-macosx_10_9_universal2.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0)\n",
      "  Using cached certifi-2025.6.15-py3-none-any.whl.metadata (2.4 kB)\n",
      "Using cached accelerate-1.8.1-py3-none-any.whl (365 kB)\n",
      "Using cached numpy-2.2.6-cp310-cp310-macosx_14_0_arm64.whl (5.3 MB)\n",
      "Using cached huggingface_hub-0.33.1-py3-none-any.whl (515 kB)\n",
      "Using cached hf_xet-1.1.5-cp37-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "Using cached fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Using cached PyYAML-6.0.2-cp310-cp310-macosx_11_0_arm64.whl (171 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "Using cached torch-2.7.1-cp310-none-macosx_11_0_arm64.whl (68.6 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached typing_extensions-4.14.0-py3-none-any.whl (43 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp310-cp310-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached psutil-7.0.0-cp36-abi3-macosx_11_0_arm64.whl (239 kB)\n",
      "Using cached requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.2-cp310-cp310-macosx_10_9_universal2.whl (201 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached certifi-2025.6.15-py3-none-any.whl (157 kB)\n",
      "Installing collected packages: mpmath, urllib3, typing-extensions, tqdm, sympy, safetensors, pyyaml, psutil, packaging, numpy, networkx, MarkupSafe, idna, hf-xet, fsspec, filelock, charset_normalizer, certifi, requests, jinja2, torch, huggingface_hub, accelerate\n",
      "\u001b[2K  Attempting uninstall: mpmath\n",
      "\u001b[2K    Found existing installation: mpmath 1.3.0\n",
      "\u001b[2K    Uninstalling mpmath-1.3.0:\n",
      "\u001b[2K      Successfully uninstalled mpmath-1.3.0\n",
      "\u001b[2K  Attempting uninstall: urllib3━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 0/23\u001b[0m [mpmath]\n",
      "\u001b[2K    Found existing installation: urllib3 2.5.00m \u001b[32m 0/23\u001b[0m [mpmath]\n",
      "\u001b[2K    Uninstalling urllib3-2.5.0:━━━━━━━━━━━━━\u001b[0m \u001b[32m 0/23\u001b[0m [mpmath]\n",
      "\u001b[2K      Successfully uninstalled urllib3-2.5.0\u001b[0m \u001b[32m 0/23\u001b[0m [mpmath]\n",
      "\u001b[2K  Attempting uninstall: typing-extensions━━━\u001b[0m \u001b[32m 0/23\u001b[0m [mpmath]\n",
      "\u001b[2K    Found existing installation: typing_extensions 4.14.023\u001b[0m [mpmath]\n",
      "\u001b[2K    Uninstalling typing_extensions-4.14.0:━━\u001b[0m \u001b[32m 0/23\u001b[0m [mpmath]\n",
      "\u001b[2K      Successfully uninstalled typing_extensions-4.14.0━━━━━━━━━━━\u001b[0m \u001b[32m 2/23\u001b[0m [typing-extensions]\n",
      "\u001b[2K  Attempting uninstall: tqdm━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/23\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Found existing installation: tqdm 4.67.1━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/23\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Uninstalling tqdm-4.67.1:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/23\u001b[0m [typing-extensions]\n",
      "\u001b[2K      Successfully uninstalled tqdm-4.67.1━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/23\u001b[0m [typing-extensions]\n",
      "\u001b[2K  Attempting uninstall: sympy━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/23\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Found existing installation: sympy 1.14.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/23\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Uninstalling sympy-1.14.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/23\u001b[0m [typing-extensions]\n",
      "\u001b[2K      Successfully uninstalled sympy-1.14.0━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/23\u001b[0m [sympy]ensions]\n",
      "\u001b[2K  Attempting uninstall: safetensors━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/23\u001b[0m [sympy]\n",
      "\u001b[2K    Found existing installation: safetensors 0.5.3━━━━━━━━━━━━\u001b[0m \u001b[32m 4/23\u001b[0m [sympy]\n",
      "\u001b[2K    Uninstalling safetensors-0.5.3:━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/23\u001b[0m [sympy]\n",
      "\u001b[2K      Successfully uninstalled safetensors-0.5.3━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/23\u001b[0m [sympy]\n",
      "\u001b[2K  Attempting uninstall: pyyaml━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/23\u001b[0m [sympy]\n",
      "\u001b[2K    Found existing installation: PyYAML 6.0.2━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/23\u001b[0m [sympy]\n",
      "\u001b[2K    Uninstalling PyYAML-6.0.2:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/23\u001b[0m [sympy]\n",
      "\u001b[2K      Successfully uninstalled PyYAML-6.0.2━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/23\u001b[0m [sympy]\n",
      "\u001b[2K  Attempting uninstall: psutil━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/23\u001b[0m [sympy]\n",
      "\u001b[2K    Found existing installation: psutil 7.0.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/23\u001b[0m [sympy]\n",
      "\u001b[2K    Uninstalling psutil-7.0.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/23\u001b[0m [sympy]\n",
      "\u001b[2K      Successfully uninstalled psutil-7.0.0━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/23\u001b[0m [sympy]\n",
      "\u001b[2K  Attempting uninstall: packagingm\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/23\u001b[0m [psutil]\n",
      "\u001b[2K    Found existing installation: packaging 25.0━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/23\u001b[0m [psutil]\n",
      "\u001b[2K    Uninstalling packaging-25.0:90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/23\u001b[0m [psutil]\n",
      "\u001b[2K      Successfully uninstalled packaging-25.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/23\u001b[0m [psutil]\n",
      "\u001b[2K  Attempting uninstall: numpym\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/23\u001b[0m [psutil]\n",
      "\u001b[2K    Found existing installation: numpy 2.2.6━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/23\u001b[0m [psutil]\n",
      "\u001b[2K    Uninstalling numpy-2.2.6:m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/23\u001b[0m [psutil]\n",
      "\u001b[2K      Successfully uninstalled numpy-2.2.6━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/23\u001b[0m [psutil]\n",
      "\u001b[2K  Attempting uninstall: networkx╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/23\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: networkx 3.4.2━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/23\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling networkx-3.4.2:m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/23\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled networkx-3.4.2━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/23\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: MarkupSafe╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/23\u001b[0m [networkx]\n",
      "\u001b[2K    Found existing installation: MarkupSafe 3.0.2━━━━━━━━━━━━━\u001b[0m \u001b[32m10/23\u001b[0m [networkx]\n",
      "\u001b[2K    Uninstalling MarkupSafe-3.0.2:m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/23\u001b[0m [networkx]\n",
      "\u001b[2K      Successfully uninstalled MarkupSafe-3.0.2━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/23\u001b[0m [networkx]\n",
      "\u001b[2K  Attempting uninstall: idna0m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/23\u001b[0m [networkx]\n",
      "\u001b[2K    Found existing installation: idna 3.10━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/23\u001b[0m [networkx]\n",
      "\u001b[2K    Uninstalling idna-3.10:90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/23\u001b[0m [networkx]\n",
      "\u001b[2K      Successfully uninstalled idna-3.10━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/23\u001b[0m [networkx]\n",
      "\u001b[2K  Attempting uninstall: hf-xet0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/23\u001b[0m [idna]\n",
      "\u001b[2K    Found existing installation: hf-xet 1.1.5━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/23\u001b[0m [idna]\n",
      "\u001b[2K    Uninstalling hf-xet-1.1.5:91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/23\u001b[0m [idna]\n",
      "\u001b[2K      Successfully uninstalled hf-xet-1.1.5━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/23\u001b[0m [idna]\n",
      "\u001b[2K  Attempting uninstall: fsspec91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/23\u001b[0m [idna]\n",
      "\u001b[2K    Found existing installation: fsspec 2025.5.1━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/23\u001b[0m [idna]\n",
      "\u001b[2K    Uninstalling fsspec-2025.5.1:╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/23\u001b[0m [idna]\n",
      "\u001b[2K      Successfully uninstalled fsspec-2025.5.1━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/23\u001b[0m [idna]\n",
      "\u001b[2K  Attempting uninstall: filelock\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/23\u001b[0m [fsspec]\n",
      "\u001b[2K    Found existing installation: filelock 3.18.0━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/23\u001b[0m [fsspec]\n",
      "\u001b[2K    Uninstalling filelock-3.18.0:[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/23\u001b[0m [fsspec]\n",
      "\u001b[2K      Successfully uninstalled filelock-3.18.0m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/23\u001b[0m [fsspec]\n",
      "\u001b[2K  Attempting uninstall: charset_normalizer\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/23\u001b[0m [fsspec]\n",
      "\u001b[2K    Found existing installation: charset-normalizer 3.4.2━━━━━\u001b[0m \u001b[32m14/23\u001b[0m [fsspec]\n",
      "\u001b[2K    Uninstalling charset-normalizer-3.4.2:\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/23\u001b[0m [fsspec]\n",
      "\u001b[2K      Successfully uninstalled charset-normalizer-3.4.2━━━━━━━\u001b[0m \u001b[32m14/23\u001b[0m [fsspec]\n",
      "\u001b[2K  Attempting uninstall: certifim\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/23\u001b[0m [fsspec]\n",
      "\u001b[2K    Found existing installation: certifi 2025.6.15━━━━━━━━━━━━\u001b[0m \u001b[32m14/23\u001b[0m [fsspec]\n",
      "\u001b[2K    Uninstalling certifi-2025.6.15:0m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/23\u001b[0m [fsspec]\n",
      "\u001b[2K      Successfully uninstalled certifi-2025.6.15━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/23\u001b[0m [fsspec]\n",
      "\u001b[2K  Attempting uninstall: requests\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/23\u001b[0m [fsspec]\n",
      "\u001b[2K    Found existing installation: requests 2.32.4━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/23\u001b[0m [fsspec]\n",
      "\u001b[2K    Uninstalling requests-2.32.4:[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/23\u001b[0m [fsspec]\n",
      "\u001b[2K      Successfully uninstalled requests-2.32.4m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/23\u001b[0m [fsspec]\n",
      "\u001b[2K  Attempting uninstall: jinja20m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/23\u001b[0m [fsspec]\n",
      "\u001b[2K    Found existing installation: Jinja2 3.1.60m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/23\u001b[0m [fsspec]\n",
      "\u001b[2K    Uninstalling Jinja2-3.1.6:0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/23\u001b[0m [fsspec]\n",
      "\u001b[2K      Successfully uninstalled Jinja2-3.1.6[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/23\u001b[0m [fsspec]\n",
      "\u001b[2K  Attempting uninstall: torch[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/23\u001b[0m [fsspec]\n",
      "\u001b[2K    Found existing installation: torch 2.7.190m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/23\u001b[0m [fsspec]\n",
      "\u001b[2K    Uninstalling torch-2.7.1:━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m20/23\u001b[0m [torch]\n",
      "\u001b[2K      Successfully uninstalled torch-2.7.1\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m20/23\u001b[0m [torch]\n",
      "\u001b[2K  Attempting uninstall: huggingface_hub━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m20/23\u001b[0m [torch]\n",
      "\u001b[2K    Found existing installation: huggingface-hub 0.33.10m━━━━━\u001b[0m \u001b[32m20/23\u001b[0m [torch]\n",
      "\u001b[2K    Uninstalling huggingface-hub-0.33.1:0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m20/23\u001b[0m [torch]\n",
      "\u001b[2K      Successfully uninstalled huggingface-hub-0.33.1[90m━━━━━\u001b[0m \u001b[32m20/23\u001b[0m [torch]\n",
      "\u001b[2K  Attempting uninstall: accelerate━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m21/23\u001b[0m [huggingface_hub]\n",
      "\u001b[2K    Found existing installation: accelerate 1.8.1╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m21/23\u001b[0m [huggingface_hub]\n",
      "\u001b[2K    Uninstalling accelerate-1.8.1:━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m21/23\u001b[0m [huggingface_hub]\n",
      "\u001b[2K      Successfully uninstalled accelerate-1.8.11m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m21/23\u001b[0m [huggingface_hub]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/23\u001b[0m [accelerate]3\u001b[0m [accelerate]]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed MarkupSafe-3.0.2 accelerate-1.8.1 certifi-2025.6.15 charset_normalizer-3.4.2 filelock-3.18.0 fsspec-2025.5.1 hf-xet-1.1.5 huggingface_hub-0.33.1 idna-3.10 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 numpy-2.2.6 packaging-25.0 psutil-7.0.0 pyyaml-6.0.2 requests-2.32.4 safetensors-0.5.3 sympy-1.14.0 torch-2.7.1 tqdm-4.67.1 typing-extensions-4.14.0 urllib3-2.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --force-reinstall \"accelerate>=0.26.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90a90db-420c-4ba2-9ac6-265f8028874e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
