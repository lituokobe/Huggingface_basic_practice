{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e647e2ec-6787-4071-9899-21b191b43cc4",
   "metadata": {},
   "source": [
    "#### - BERT/GPT/Transfoer is suitable for classification, but it is very big. We do fine-tuning at most, barely train a new one.\n",
    "#### - backbone for feature extraction, pass the task model (usually neuron network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01ec7797-b590-4f85-8348-cb1a2721fbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from transformers import BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7ee8e27-17fd-43dd-87b2-1dbe073d9646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26921f1ff4d1418e9c182b7e85f8750c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e5ff4f87904e648effbe47af2a4cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb694403d3fc44d5aee8c196b5c09722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='bert-base-chinese', vocab_size=21128, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ffa8864-387c-477a-b409-9f4689b87e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trail to encode\n",
    "out = tokenizer.batch_encode_plus(\n",
    "    batch_text_or_text_pairs = ['心态是最正规的。人生是很权威的。'],\n",
    "    truncation = True,\n",
    "    padding = 'max_length',\n",
    "    max_length = 15,\n",
    "    return_tensors = 'pt',\n",
    "    return_length = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b97672d7-b76d-4493-abf5-0adf4410e560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids tensor([[ 101, 2552, 2578, 3221, 3297, 3633, 6226, 4638,  511,  782, 4495, 3221,\n",
      "         2523, 3326,  102]])\n",
      "token_type_ids tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "length tensor([15])\n",
      "attention_mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "for k, v in out.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7ed6bd3-4eaf-4fec-b6e3-60851dc189dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 心 态 是 最 正 规 的 。 人 生 是 很 权 [SEP]'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(out['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77c5021-0d83-45c2-964d-85a2ddd36f11",
   "metadata": {},
   "source": [
    "## Define dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ec599de-842e-4b86-b37f-2b4645deeeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split):\n",
    "        self.dataset = load_from_disk('../ChnSentiCorp/')[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        text = self.dataset[i]['text']\n",
    "        label = self.dataset[i]['label']\n",
    "\n",
    "        return text, label\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0be2c0cd-d19d-44a9-a19b-64d1fd7ae1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5038fab2-eb08-4502-9dc5-ec47bb0d652a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9600"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74cbd93b-09bc-46ac-a279-7c611e1ac751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('非常不错，服务很好，位于市中心区，交通方便，不过价格也高！', 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5c49724-5f69-4f3e-937a-d3f33dc39835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Dataset at 0x129701f60>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00ef303f-7874-46ee-bdfd-5cc2c22ed87f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.Dataset"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752d8ac6-a7db-4a41-b2a5-4005f68d3944",
   "metadata": {},
   "source": [
    "Define device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f840faf-2aaa-4e56-98b3-e570dfd52d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c992418-a6e8-4776-ad05-b8ff1a0afc78",
   "metadata": {},
   "source": [
    "## Function to organize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5c5c312a-425e-4219-928b-4a4fdf86f7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    sents = [i[0] for i in data]\n",
    "    labels = [i[1] for i in data]\n",
    "\n",
    "    data = tokenizer.batch_encode_plus(batch_text_or_text_pairs = sents,\n",
    "                                       truncation = True,\n",
    "                                       padding = 'max_length',\n",
    "                                       max_length = 500,\n",
    "                                       return_tensors = 'pt', # 'pt' means PyTorch, it could be 'np' or 'tf' as well\n",
    "                                       return_length = True)\n",
    "\n",
    "    input_ids = data['input_ids']\n",
    "    attention_mask = data['attention_mask']\n",
    "    token_type_ids = data['token_type_ids']\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    token_type_ids = token_type_ids.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    return input_ids, attention_mask, token_type_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ecdb90ec-4809-442b-8ae4-2c9f6cfa59d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    ('科比就像奥尼尔的正宫大老婆，虽然不是很满意，但好歹是患难与共，也一起收获过很多快乐和成就的。', 1),\n",
    "    ('韦德可以说是后来的续弦，虽然也是明媒正娶，年龄、地位和气势总归差距大了，合作愉快，但内心总觉得缺少了什么。', 0),\n",
    "    ('至于便士，那妥妥就是鲨鱼心中的白月光了。', 1),\n",
    "    ('奥尼尔这两年上镜，很容易流泪，声音也很低沉，语速很慢，我怀疑他有抑郁症。', 0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "35c9c871-014c-4dcc-9e1e-a5440a0a2d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, attention_mask, token_type_ids, labels = collate_fn(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "94f78551-6909-4e07-b341-53fff7af58d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "16b2489f-7569-4c4e-b567-1d1481b4eaa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 0], device='mps:0')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d07c47be-cc6b-4701-b0fe-5c3f31b2b3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 500]) torch.Size([4, 500]) torch.Size([4, 500])\n"
     ]
    }
   ],
   "source": [
    "print(input_ids.shape, attention_mask.shape, token_type_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dfd40f2c-ef25-4f28-80ad-908d3bfaac0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloader, only for practice purpose\n",
    "loader = torch.utils.data.DataLoader(dataset = dataset,\n",
    "                                     batch_size = 16,\n",
    "                                     collate_fn = collate_fn,\n",
    "                                     shuffle = True,\n",
    "                                     drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "61963bd8-7c41-4b11-97a4-b7b8420e93c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4be79d8c-12fe-4c3b-b396-81b7f0460f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 500]),\n",
       " torch.Size([16, 500]),\n",
       " torch.Size([16, 500]),\n",
       " tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1], device='mps:0'))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(loader):\n",
    "#     break\n",
    "\n",
    "input_ids.shape, attention_mask.shape, token_type_ids.shape, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bfdf0a31-f9ee-4dda-bc62-de7734b189fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a635645cabb4a528469b0e216cd18c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/412M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load pretrained model\n",
    "from transformers import BertModel\n",
    "\n",
    "pretrained = BertModel.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8937c825-4ed8-4ca9-afbb-ba0b6e651be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102267648"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see the parameter amount\n",
    "sum(i.numel() for i in pretrained.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa43c93-c393-4a59-930c-7b61ef25d214",
   "metadata": {},
   "source": [
    "The `.numel()` method in PyTorch (and also available in some other frameworks like TensorFlow and NumPy via `.size` or `.size().prod()`) returns the total number of elements in a tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc20da2d-00c3-4bba-9a12-adc5ac9c3981",
   "metadata": {},
   "source": [
    "## Freeze parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "749364be-6a49-4131-805a-d0b8cea35433",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in pretrained.parameters():\n",
    "    param.requires_grad_(False)\n",
    "#cannot do derevatives and back-propagation any more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bfa1db9a-0394-4aad-a197-ba7d6c19f32e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "12b735c4-e761-4c91-a64b-4344413b4136",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pretrained(input_ids = input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "87f95e3d-39bb-460f-ab64-24c7ef7ac0a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 500, 768])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effecae9-fb85-40f1-b0e6-83b716738cba",
   "metadata": {},
   "source": [
    "- 16 in this batch\n",
    "- 500 Sequence length\tEach sequence was padded or truncated to 500 tokens long\n",
    "- 768 Hidden size\tEach token is represented by a 768-dimensional embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "54fc94a4-186f-45c0-9d6b-9e4a4af75d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 768])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.last_hidden_state[:,0, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c64b75d5-cde2-4d3d-af6e-6113d97d8b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.last_hidden_state[0,:, 0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d913561d-83c5-476c-8d3d-65158fa18975",
   "metadata": {},
   "source": [
    "## Define a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f2867811-2d20-42b5-bd2a-ba4fca1f2993",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(in_features = 768, out_features = 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        # with torch.no_grad():\n",
    "        #     out = pretrained(input_ids = input_ids,\n",
    "        #                      attention_mask = attention_mask,\n",
    "        #                      token_type_ids = token_type_ids)\n",
    "        #     out = self.fc(out.last_hidden_state[:,0]) #equal to out.last_hidden_state[:, 0, :], Linear layer expects 2 dimensional data\n",
    "        #     out = out.softmax(dim = 1)\n",
    "        #     return out\n",
    "\n",
    "        out = pretrained(input_ids = input_ids,\n",
    "                         attention_mask = attention_mask,\n",
    "                         token_type_ids = token_type_ids)\n",
    "        out = self.fc(out.last_hidden_state[:,0]) #equal to out.last_hidden_state[:, 0, :], Linear layer expects 2 dimensional data\n",
    "        out = out.softmax(dim = 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e13b12c-6486-43a7-9c6c-c2cafb196d07",
   "metadata": {},
   "source": [
    "Why last_hidden_state[:,0]?\n",
    "- In models like BERT, the first token (position 0) is a synthetic token called [CLS], short for Classification.\n",
    "- It’s not part of the original input sentence—it’s added by the tokenizer.\n",
    "- During training, the model learns to pack a summary of the entire sequence into the [CLS] token's embedding.\n",
    "- [CLS] becomes the attention magnet During training for tasks like classification, the model is explicitly trained to make the [CLS] vector capture all the important information. Because the loss function is applied directly to it—it’s what feeds the classifier head.\n",
    "- It’s trained to absorb and summarize information from the whole sequence. The loss function in classification tasks is directly tied to its output during pretraining and fine-tuning. It's the one doing the most listening when trained for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e66caa0b-12d3-407a-88db-550aca8414a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2324749c-3dc2-40e8-a00f-7fbcba0c0f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (fc): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "34464e38-af0f-4664-a900-8715313ab01a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6226, 0.3774],\n",
       "        [0.5881, 0.4119],\n",
       "        [0.4956, 0.5044],\n",
       "        [0.6835, 0.3165],\n",
       "        [0.6196, 0.3804],\n",
       "        [0.7332, 0.2668],\n",
       "        [0.6828, 0.3172],\n",
       "        [0.6107, 0.3893],\n",
       "        [0.5979, 0.4021],\n",
       "        [0.5717, 0.4283],\n",
       "        [0.5741, 0.4259],\n",
       "        [0.6400, 0.3600],\n",
       "        [0.5127, 0.4873],\n",
       "        [0.7230, 0.2770],\n",
       "        [0.6481, 0.3519],\n",
       "        [0.6398, 0.3602]], device='mps:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trail\n",
    "model(input_ids, attention_mask, token_type_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cd276a-28b1-4098-bd69-d5e0914d0508",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a001c545-efff-4b7e-9416-7f86edaad2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from transformers.optimization import get_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1fac628f-99af-4bd1-b8f0-46ee93f0c54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    optimizer =  AdamW(model.parameters(), lr = 0.0001)\n",
    "    \n",
    "    #loss function\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    scheduler = get_scheduler(name = 'linear',\n",
    "                              num_warmup_steps = 0,\n",
    "                              num_training_steps = len(loader),\n",
    "                              optimizer = optimizer)\n",
    "    model.train()\n",
    "\n",
    "    for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(loader):\n",
    "        out = model(input_ids, attention_mask, token_type_ids)\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if i%10 ==0:\n",
    "            out = out.argmax(dim = 1)\n",
    "            accuracy = (out==labels).sum().item()/len(labels)\n",
    "            lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "            print(i, loss.item(), lr, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "010f6fd9-00f0-48f1-81f2-81a91af578d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7071318626403809 9.983333333333334e-05 0.4375\n",
      "10 0.7115724086761475 9.816666666666668e-05 0.375\n",
      "20 0.7273609638214111 9.65e-05 0.3125\n",
      "30 0.6518293023109436 9.483333333333334e-05 0.8125\n",
      "40 0.6847739815711975 9.316666666666666e-05 0.5625\n",
      "50 0.6582504510879517 9.15e-05 0.75\n",
      "60 0.6269950270652771 8.983333333333334e-05 0.8125\n",
      "70 0.6505166888237 8.816666666666668e-05 0.5625\n",
      "80 0.6002245545387268 8.65e-05 0.9375\n",
      "90 0.6611295938491821 8.483333333333334e-05 0.625\n",
      "100 0.6146636009216309 8.316666666666666e-05 0.75\n",
      "110 0.6305767893791199 8.15e-05 0.75\n",
      "120 0.6338016986846924 7.983333333333334e-05 0.8125\n",
      "130 0.6113054752349854 7.816666666666666e-05 0.8125\n",
      "140 0.591750979423523 7.65e-05 0.8125\n",
      "150 0.5957347750663757 7.483333333333333e-05 0.875\n",
      "160 0.6201099157333374 7.316666666666668e-05 0.75\n",
      "170 0.6292654871940613 7.15e-05 0.625\n",
      "180 0.5875069499015808 6.983333333333334e-05 0.875\n",
      "190 0.5883765816688538 6.816666666666667e-05 0.8125\n",
      "200 0.5737649202346802 6.65e-05 0.875\n",
      "210 0.5999044179916382 6.483333333333333e-05 0.6875\n",
      "220 0.614923357963562 6.316666666666668e-05 0.8125\n",
      "230 0.6020519137382507 6.15e-05 0.8125\n",
      "240 0.6017087697982788 5.983333333333334e-05 0.6875\n",
      "250 0.6411841511726379 5.8166666666666667e-05 0.5\n",
      "260 0.5866502523422241 5.65e-05 0.75\n",
      "270 0.5203119516372681 5.4833333333333336e-05 0.9375\n",
      "280 0.5955249667167664 5.316666666666667e-05 0.8125\n",
      "290 0.5502725839614868 5.1500000000000005e-05 0.75\n",
      "300 0.5592716932296753 4.9833333333333336e-05 0.8125\n",
      "310 0.5684300065040588 4.8166666666666674e-05 0.8125\n",
      "320 0.595487654209137 4.6500000000000005e-05 0.8125\n",
      "330 0.5343652963638306 4.483333333333333e-05 0.9375\n",
      "340 0.5316964983940125 4.316666666666667e-05 0.875\n",
      "350 0.512657642364502 4.15e-05 0.875\n",
      "360 0.5421531796455383 3.983333333333333e-05 0.875\n",
      "370 0.48270994424819946 3.816666666666667e-05 0.9375\n",
      "380 0.5256903171539307 3.65e-05 0.9375\n",
      "390 0.5738121271133423 3.483333333333334e-05 0.875\n",
      "400 0.454307496547699 3.316666666666667e-05 0.9375\n",
      "410 0.5229530334472656 3.15e-05 0.9375\n",
      "420 0.5091376900672913 2.9833333333333335e-05 0.9375\n",
      "430 0.526699423789978 2.816666666666667e-05 0.8125\n",
      "440 0.5989137291908264 2.6500000000000004e-05 0.75\n",
      "450 0.46344396471977234 2.4833333333333335e-05 1.0\n",
      "460 0.5831571817398071 2.3166666666666666e-05 0.8125\n",
      "470 0.5594823360443115 2.15e-05 0.8125\n",
      "480 0.5755589008331299 1.9833333333333335e-05 0.75\n",
      "490 0.4849691092967987 1.8166666666666667e-05 0.9375\n",
      "500 0.5703648924827576 1.65e-05 0.75\n",
      "510 0.5754029750823975 1.4833333333333336e-05 0.75\n",
      "520 0.47786927223205566 1.3166666666666665e-05 0.9375\n",
      "530 0.5185670852661133 1.1500000000000002e-05 0.875\n",
      "540 0.5265358686447144 9.833333333333333e-06 0.75\n",
      "550 0.5658259391784668 8.166666666666668e-06 0.875\n",
      "560 0.5019792914390564 6.5000000000000004e-06 1.0\n",
      "570 0.5149033069610596 4.833333333333333e-06 0.8125\n",
      "580 0.5627577304840088 3.166666666666667e-06 0.75\n",
      "590 0.5110518932342529 1.5e-06 0.8125\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e890493e-7b7b-4fba-8420-27314ede5347",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "35bb5052-a0a3-4b49-b397-7a1fd5fa2c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    loader_test = torch.utils.data.DataLoader(dataset = Dataset('test'),\n",
    "                                                        batch_size = 32,\n",
    "                                                        collate_fn = collate_fn,\n",
    "                                                        shuffle = True,\n",
    "                                                        drop_last = True\n",
    "                                              )\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(loader_test):\n",
    "        if i == 5:\n",
    "            break\n",
    "        print(i)\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "        out = out.argmax(dim = 1)\n",
    "        correct +=(out==labels).sum().item()\n",
    "        total += len(labels)\n",
    "        print(correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0cd294b8-aeab-414c-b09d-ad10d7b010b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.78125\n",
      "1\n",
      "0.8125\n",
      "2\n",
      "0.84375\n",
      "3\n",
      "0.875\n",
      "4\n",
      "0.875\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4c50a4-a387-4d66-8daf-e9fe1bc0818a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
